# Lab 2: Wall Following
The instructions to this lab can be found at [this link](http://coecsl.ece.illinois.edu/se498/SE498%20Lab2.pdf). By the end of the lab, students have created and tuned an algorithm to make a robot car find and follow a wall.  

## Edited Files
The bulk of the work for this lab was completed in the the <code>timerCallback()</code> function of [wallfollow.cpp](https://github.com/monk200/Mobile_Robotics_with_ROS/blob/main/src/SE498Lab2_skeleton/src/wallfollow.cpp). Some edits were made to a [settings file](https://github.com/monk200/Mobile_Robotics_with_ROS/blob/main/src/ROS_Serial/cardriver/yaml/setting.yaml) in the [ROS_Serial](https://github.com/monk200/Mobile_Robotics_with_ROS/tree/main/src/ROS_Serial) directory. [wallfollow.hpp](https://github.com/monk200/Mobile_Robotics_with_ROS/blob/main/src/SE498Lab2_skeleton/src/wallfollow.hpp) was also edited at times, but only for turning on certain sensors to test and tune them. 

## Hardware
The robot car was provided to students by the course but its operations were explained to us. The robot uses a STM32F4 real-time microcontroller for its low-level control and a Raspberry Pi 3 B+ as its high-level controller. The low-level controller samples an IMU, five IR proximity sensors, and wheel encoders at about 1000Hz then sends this information to the high-level controller over UART. The high-level controller runs ROS at about 20-50Hz and processes the Lidar and camera system, then combines the readings to determine control actions. The IR sensors are positioned so that there is a sensor pointing diagonally off each corner and one in the front center of the car.  

<p align="center"><img src="https://github.com/monk200/Mobile_Robotics_with_ROS/blob/main/src/SE498Lab2_skeleton/ROS%20robot.PNG" height="250em" /></p>

## Running and Testing
To start, the STM32F4 low-level controller needs to be connected to your computer via USB cable and the file [./main/src/ROS_Serial/cardriver/yaml/setting.yaml](https://github.com/monk200/Mobile_Robotics_with_ROS/blob/main/src/ROS_Serial/cardriver/yaml/setting.yaml) needs to be edited to reflect the correct port that the controller is plugged into. The port can be verified by typing <code>roslaunch cardriver enumeratePorts.launch</code> into the terminal, which will return a list of ports. Next, the sensors on the car can be configured by uncommenting the corresponding sensor set on lines 11-14 in [wallfollow.hpp](https://github.com/monk200/Mobile_Robotics_with_ROS/blob/main/src/SE498Lab2_skeleton/src/wallfollow.hpp). Then the code can be compiled by navigating to your workspace, typing <code>catkin_make</code>, <code>cd source devel/setup.bash</code>, and <code>rosrun lab2 lab2node</code>. Re-commenting the sensor sets and running the code again will work to run the wall following algorithm when using SSH to control the Raspberry Pi high-level controller. 

## Output
Unfortunately, I don't have a video to demonstrate the completed lab. During the course, I was able to get the car to follow a left wall fairly accurately but slightly too close because sometimes the robot would bump into the wall when making small adjustments. A possible solution to this is to do more fine tuning of the sensor measurments or create more detailed instructions for what to do when the car is following the wall. At the moment, the IR sensor by the back wheel seems underutilized. I wasn't able to tune the left wall follow to the level I wanted to, and so I also didn't get around to tuning the right wall follow at all so at the moment it is just mirrored from the left's algoirthm.  
