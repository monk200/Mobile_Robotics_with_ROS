# Lab 4: Image Processing with OpenCV
The instructions to this lab can be found at [this link](http://coecsl.ece.illinois.edu/se498/SE498%20Lab4.pdf). The intended result of this lab was to implement a line-following algorithm on the same robot car as [SE498Lab2_skeleton](https://github.com/monk200/Mobile_Robotics_with_ROS/tree/main/src/SE498Lab2_skeleton), but this final step wasn't reached because of Covid. Instead, by the end of the lab students have created a program that performs edge detection on a live video, draws crosshairs on that live video, and is able to save stills from the video feed on keypress.  

## Edited Files
The solution to this lab was created by iteratively solving the problem in [load_image.cpp](https://github.com/monk200/Mobile_Robotics_with_ROS/blob/main/src/se498_lab4/src/load_image.cpp), [read_camera.cpp](https://github.com/monk200/Mobile_Robotics_with_ROS/blob/main/src/se498_lab4/src/read_camera.cpp), and finally [image_pub_sub.cpp](https://github.com/monk200/Mobile_Robotics_with_ROS/blob/main/src/se498_lab4/src/image_pub_sub.cpp). The first file was used to take an input image from the [images](https://github.com/monk200/Mobile_Robotics_with_ROS/tree/main/src/se498_lab4/images) directory and produce an edge detection image by applying a Gaussian blur and then using OpenCV's Canny function. The input image can be changed to play with how different values in the functions affect the quality of the output in different situations. The second file performs edge detection on each frame of the live video from the computer's camera. This leads to the last file running ROS and performing edge detection, along with overlaying a square and crosshair at the center of the screen. As an additional functionality, if the user presses the "s" key while the video is running, it will save the current frame to the [images](https://github.com/monk200/Mobile_Robotics_with_ROS/tree/main/src/se498_lab4/images) directory.  

## Running and Testing
Regardless of which file is meant to be run, start by typing <code>catkin_make</code>, <code>source devel/setup.bash</code>, and then <code>roscore</code> in the terminal. Then open a second terminal and be sure to always type <code>source devel/setup.bash</code>. Depending on which program you indend to run, run either <code>rosrun lab4 load_image_cpp</code>, <code>rosrun lab4 read_camera_cpp</code>, or <code>roslaunch lab4 image_pub_sub.launch</code>.  

## Output
Starting with the output of [load_image.cpp](https://github.com/monk200/Mobile_Robotics_with_ROS/blob/main/src/se498_lab4/src/load_image.cpp), the original image can be seen below along with the edge detection output without (left) and with the Gaussian blur (right). Because the texture on the ground in the original picture, the blur is essential to getting good edge detection. Other cases have similar issues of needing the blur to remove edge detection on irrelivant patterns, but the intensity of the blur (labeled as the maximum kernel length) may need to be increased or decreased to remove or add detail.  

<p align="center"><img src="https://github.com/monk200/Mobile_Robotics_with_ROS/blob/main/src/se498_lab4/images/new_image.jpg" alt="Original Image" width="400"/></p>
<p align="center"><img src="https://github.com/monk200/Mobile_Robotics_with_ROS/blob/main/src/se498_lab4/images/unblurred_edge_image.jpg" alt="Edge Detection without Gaussian Blur" width="400"/> <img src="https://github.com/monk200/Mobile_Robotics_with_ROS/blob/main/src/se498_lab4/images/copy_image.jpg" alt="Edge Detection with Gaussian Blur" width="400"/></p>

The output of [read_camera.cpp](https://github.com/monk200/Mobile_Robotics_with_ROS/blob/main/src/se498_lab4/src/read_camera.cpp) simply performed edge detection on a live feed from the camera. I didn't save an output because a similar functionality can be seen in [image_pub_sub.cpp](https://github.com/monk200/Mobile_Robotics_with_ROS/blob/main/src/se498_lab4/src/image_pub_sub.cpp). This final file performs edge detection on a live feed and used OpenCV's drawing tools to draw a 100x100px box and crosshairs at the center of the video. In addition to this, if the user presses the "s" key while the script is running, the current frame will be saved. For this portion of the lab I significantly reduced the maximum kernal length to be able to get good detail on my face but details are lost from the background of my white-walled and well-lit room. An example of this working can be seen below: 

<p align="center"><img src="https://github.com/monk200/Mobile_Robotics_with_ROS/blob/main/src/se498_lab4/images/video_still.jpg" alt="Live Video Edge Detection" width="500"/></p>
